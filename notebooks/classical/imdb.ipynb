{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Reviews (Classical)\n",
    "\n",
    "This notebook trains and evaluates classical baselines for the IMDb Reviews sentiment classification task. Note that this is a text classification task.\n",
    "You can find information about the dataset at https://www.tensorflow.org/datasets/catalog/imdb_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T10:06:25.805947Z",
     "iopub.status.busy": "2023-11-03T10:06:25.805811Z",
     "iopub.status.idle": "2023-11-03T10:06:44.038024Z",
     "shell.execute_reply": "2023-11-03T10:06:44.037532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 03:06:28.166936: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-03 03:06:28.166966: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-03 03:06:28.166993: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-03 03:06:31.149585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')  # Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.random.set_seed(42)  # For reproducibility.\n",
    "\n",
    "from quantum_transformers.datasets import get_imdb_dataloaders\n",
    "from quantum_transformers.training import train_and_evaluate\n",
    "from quantum_transformers.transformers import Transformer\n",
    "\n",
    "data_dir = '/global/cfs/cdirs/m4392/salcc/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are trained using the following devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T10:06:44.040283Z",
     "iopub.status.busy": "2023-11-03T10:06:44.039960Z",
     "iopub.status.idle": "2023-11-03T10:06:44.285658Z",
     "shell.execute_reply": "2023-11-03T10:06:44.285125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu:0 NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "for d in jax.devices():\n",
    "    print(d, d.device_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how big is the vocabulary, and see an example of one example review (both in tokenized and raw form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T10:06:44.287666Z",
     "iopub.status.busy": "2023-11-03T10:06:44.287453Z",
     "iopub.status.idle": "2023-11-03T10:08:25.501601Z",
     "shell.execute_reply": "2023-11-03T10:08:25.501040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinalities (train, val, test): 22500 2500 25000\n",
      "Vocabulary size: 19769\n",
      "[  140   198  2023    98  2191   313   113  3086   658    16     5  6662\n",
      "     5    99   536   120    97   237   198    17    95   317  1105    98\n",
      "  1520   376   175    42   836    16  4251  3272    15   110   300   319\n",
      "   101  3642    17    95  2266    97   103   783    99   114    98  2362\n",
      "    98  1224   147    42   908   317    15   110   341    98  6505  5022\n",
      "    95  1471    16   851  2063   739    17    31   100    18    33    31\n",
      "   100    18    33   106   247    15   668  3681   106    42   146  7141\n",
      "   186    97   864   117   246    17   181    97    95   194   116 12160\n",
      "   113  1404    15    96   499   176   123    10   248   341   272  1691\n",
      "  3271    17  8331  8412   137  2422   392  3179    98   119    42  9449\n",
      "    15   588    98  9690    96  1223    17   150  8941    98  8412    15\n",
      "   110   117   405    97  3179   281   155   588    98  1223    95  9229\n",
      "   908    16  3244    17  3143    15   101    10    60    42  1543   328\n",
      "    17  4998   103    98  8412    10    60   143    10   433    10   644\n",
      "   129   102     5    95  5663     5   101   281    98  2148   115    99\n",
      "   322   143   122   436   102  2612  2793  1662    17    31   100    18\n",
      "    33    31   100    18    33    95   246   116    95   382  1218  2575\n",
      "   121   389    17   130   116  1074   207   444   104   841   135    16\n",
      "   352    95   458    17  4080  3548    15  1847 14011    15    96  2929\n",
      " 14183   121   118  1906   360   148   465    98   157   990  8532    15\n",
      "   110   123   116   121   871    17  1297   128  8412    15  2647 18328\n",
      "    96  6222  8930   295    42  1008    15  1543  1822    98   157   644\n",
      "    17    31   100    18    33    31   100    18    33    50  1921   368\n",
      "   214   240     5  6662     5   176    50  5325   101   105    42   111\n",
      "   134    42  1613   369    15   102    95  7480  2294   380    17   523\n",
      "    15    95   154  3246   279   392  3179    15   110   101   137  1317\n",
      "   256    98   170   108   117  1710   705    17   787    15   103   111\n",
      "    99    42   682   820    15    96   106   231    16   101    10    60\n",
      "   417    17    31   100    18    33    31   100    18    33   758    29\n",
      "    26    17    24   135    97   250     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "some films manage to survive almost on originality alone - \" wonderland \" is certainly one of those films . the script manages to throw everything into a near - fever pitch , but without making it incoherent . the speed of this thriller is not to chosen to cover up a weak script , but rather to accurately reflect the drug - addled reality . < br / > < br / > as director , james cox as a very peculiar way of working his actors . most of the characters are perpetually on edge , and often because they ' re rather quite ugly personalities . val kilmer has described john holmes to be a hustler , able to manipulate and control . no offense to kilmer , but his version of holmes seems only able to control the drastically weak - minded . nonetheless , it ' s a stunning performance . comparing this to kilmer ' s more ' hollywood ' roles like in \" the saint \" it seems to prove he is far more at home in gritty indie flicks . < br / > < br / > the actors are the main force holding all together . there are various little performances that stand out - especially the women . carrie fisher , kate bosworth , and lisa kudrow all have limited screen time next to their male counterparts , but they are all fantastic . aside from kilmer , ted levine and dylan mcdermott give a weird , stunning energy to their roles . < br / > < br / > i originally put off watching \" wonderland \" because i assumed it was a film about a porn actor , in the strictest sense . yes , the story revolves around john holmes , but it has literally nothing to do with his professional career . basically , this film is a murder mystery , and as such - it ' s excellent . < br / > < br / > rating : 7 . 5 out of 10 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "(imdb_train_dataloader, imdb_val_dataloader, imdb_test_dataloader), vocab, tokenizer = get_imdb_dataloaders(batch_size=32, data_dir=data_dir, max_vocab_size=20_000, max_seq_len=512)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "first_batch = next(iter(imdb_train_dataloader))\n",
    "print(first_batch[0][0])\n",
    "print(' '.join(map(bytes.decode, tokenizer.detokenize(first_batch[0])[0].numpy().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a relatively big Transformer that obtains a good AUC score on the test set (hyperparameters found by random hyperparameter search). Note however that this model size is too big to be replicated on a quantum computer currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T10:08:25.503456Z",
     "iopub.status.busy": "2023-11-03T10:08:25.503303Z",
     "iopub.status.idle": "2023-11-03T10:10:57.313111Z",
     "shell.execute_reply": "2023-11-03T10:10:57.312561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters = 1382594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch   1/30: 100%|██████████| 703/703 [00:10<00:00, 67.04batch/s, Loss = 0.5424, AUC = 81.85%] \n",
      "Epoch   2/30: 100%|██████████| 703/703 [00:04<00:00, 158.05batch/s, Loss = 0.3482, AUC = 92.99%]\n",
      "Epoch   3/30: 100%|██████████| 703/703 [00:04<00:00, 160.02batch/s, Loss = 0.3267, AUC = 94.52%]\n",
      "Epoch   4/30: 100%|██████████| 703/703 [00:04<00:00, 157.67batch/s, Loss = 0.3122, AUC = 94.88%]\n",
      "Epoch   5/30: 100%|██████████| 703/703 [00:04<00:00, 157.65batch/s, Loss = 0.3847, AUC = 94.34%]\n",
      "Epoch   6/30: 100%|██████████| 703/703 [00:04<00:00, 157.69batch/s, Loss = 0.4091, AUC = 94.04%]\n",
      "Epoch   7/30: 100%|██████████| 703/703 [00:04<00:00, 158.37batch/s, Loss = 0.4692, AUC = 93.88%]\n",
      "Epoch   8/30: 100%|██████████| 703/703 [00:04<00:00, 158.76batch/s, Loss = 0.5167, AUC = 94.00%]\n",
      "Epoch   9/30: 100%|██████████| 703/703 [00:04<00:00, 152.83batch/s, Loss = 0.5313, AUC = 94.17%]\n",
      "Epoch  10/30: 100%|██████████| 703/703 [00:04<00:00, 159.44batch/s, Loss = 0.6408, AUC = 93.74%]\n",
      "Epoch  11/30: 100%|██████████| 703/703 [00:04<00:00, 157.36batch/s, Loss = 0.5612, AUC = 94.13%]\n",
      "Epoch  12/30: 100%|██████████| 703/703 [00:04<00:00, 159.58batch/s, Loss = 0.6525, AUC = 94.15%]\n",
      "Epoch  13/30: 100%|██████████| 703/703 [00:04<00:00, 158.68batch/s, Loss = 0.7895, AUC = 94.06%]\n",
      "Epoch  14/30: 100%|██████████| 703/703 [00:04<00:00, 158.08batch/s, Loss = 0.7245, AUC = 94.12%]\n",
      "Epoch  15/30: 100%|██████████| 703/703 [00:04<00:00, 159.10batch/s, Loss = 0.8017, AUC = 93.41%]\n",
      "Epoch  16/30: 100%|██████████| 703/703 [00:04<00:00, 160.21batch/s, Loss = 0.7206, AUC = 93.73%]\n",
      "Epoch  17/30: 100%|██████████| 703/703 [00:04<00:00, 157.73batch/s, Loss = 0.7853, AUC = 93.51%]\n",
      "Epoch  18/30: 100%|██████████| 703/703 [00:04<00:00, 158.89batch/s, Loss = 0.9374, AUC = 93.25%]\n",
      "Epoch  19/30: 100%|██████████| 703/703 [00:04<00:00, 156.32batch/s, Loss = 0.7386, AUC = 93.86%]\n",
      "Epoch  20/30: 100%|██████████| 703/703 [00:04<00:00, 157.25batch/s, Loss = 0.8213, AUC = 92.70%]\n",
      "Epoch  21/30: 100%|██████████| 703/703 [00:04<00:00, 159.12batch/s, Loss = 0.8942, AUC = 93.36%]\n",
      "Epoch  22/30: 100%|██████████| 703/703 [00:04<00:00, 157.43batch/s, Loss = 0.7997, AUC = 93.05%]\n",
      "Epoch  23/30: 100%|██████████| 703/703 [00:04<00:00, 158.88batch/s, Loss = 0.8660, AUC = 92.29%]\n",
      "Epoch  24/30: 100%|██████████| 703/703 [00:04<00:00, 159.04batch/s, Loss = 0.8794, AUC = 92.85%]\n",
      "Epoch  25/30: 100%|██████████| 703/703 [00:04<00:00, 156.52batch/s, Loss = 1.3482, AUC = 89.88%]\n",
      "Epoch  26/30: 100%|██████████| 703/703 [00:04<00:00, 158.77batch/s, Loss = 1.0491, AUC = 91.92%]\n",
      "Epoch  27/30: 100%|██████████| 703/703 [00:04<00:00, 159.04batch/s, Loss = 1.0851, AUC = 92.26%]\n",
      "Epoch  28/30: 100%|██████████| 703/703 [00:04<00:00, 158.34batch/s, Loss = 0.8587, AUC = 93.23%]\n",
      "Epoch  29/30: 100%|██████████| 703/703 [00:04<00:00, 158.39batch/s, Loss = 1.2120, AUC = 92.21%]\n",
      "Epoch  30/30: 100%|██████████| 703/703 [00:04<00:00, 158.95batch/s, Loss = 1.1790, AUC = 92.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time = 139.41s, best validation AUC = 94.88% at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 781/781 [00:05<00:00, 147.19batch/s, Loss = 0.3632, AUC = 93.18%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array(0.36320385, dtype=float32),\n",
       " 93.1777571685361,\n",
       " array([0.        , 0.        , 0.        , ..., 0.99959994, 0.99975996,\n",
       "        1.        ]),\n",
       " array([0.00000000e+00, 8.00384184e-05, 7.20345766e-04, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(num_tokens=len(vocab), max_seq_len=512, num_classes=2, hidden_size=64, num_heads=2, num_transformer_blocks=4, mlp_hidden_size=32)\n",
    "train_and_evaluate(model, imdb_train_dataloader, imdb_val_dataloader, imdb_test_dataloader, num_classes=2, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a smaller model which could be run on a quantum computer. Note that the number of parameters is much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T10:10:57.315003Z",
     "iopub.status.busy": "2023-11-03T10:10:57.314852Z",
     "iopub.status.idle": "2023-11-03T10:13:04.785070Z",
     "shell.execute_reply": "2023-11-03T10:13:04.784602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters = 163866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch   1/30: 100%|██████████| 703/703 [00:08<00:00, 81.53batch/s, Loss = 0.6908, AUC = 53.89%] \n",
      "Epoch   2/30: 100%|██████████| 703/703 [00:03<00:00, 184.13batch/s, Loss = 0.6870, AUC = 57.98%]\n",
      "Epoch   3/30: 100%|██████████| 703/703 [00:03<00:00, 184.61batch/s, Loss = 0.5736, AUC = 79.28%]\n",
      "Epoch   4/30: 100%|██████████| 703/703 [00:03<00:00, 182.53batch/s, Loss = 0.4744, AUC = 85.89%]\n",
      "Epoch   5/30: 100%|██████████| 703/703 [00:03<00:00, 182.43batch/s, Loss = 0.4941, AUC = 89.21%]\n",
      "Epoch   6/30: 100%|██████████| 703/703 [00:03<00:00, 184.32batch/s, Loss = 0.3875, AUC = 90.96%]\n",
      "Epoch   7/30: 100%|██████████| 703/703 [00:03<00:00, 185.33batch/s, Loss = 0.3723, AUC = 92.12%]\n",
      "Epoch   8/30: 100%|██████████| 703/703 [00:03<00:00, 184.15batch/s, Loss = 0.4038, AUC = 91.96%]\n",
      "Epoch   9/30: 100%|██████████| 703/703 [00:03<00:00, 184.59batch/s, Loss = 0.3936, AUC = 93.07%]\n",
      "Epoch  10/30: 100%|██████████| 703/703 [00:03<00:00, 185.92batch/s, Loss = 0.4683, AUC = 93.18%]\n",
      "Epoch  11/30: 100%|██████████| 703/703 [00:03<00:00, 186.39batch/s, Loss = 0.4601, AUC = 93.16%]\n",
      "Epoch  12/30: 100%|██████████| 703/703 [00:03<00:00, 187.51batch/s, Loss = 0.5798, AUC = 93.03%]\n",
      "Epoch  13/30: 100%|██████████| 703/703 [00:03<00:00, 186.48batch/s, Loss = 0.6385, AUC = 93.03%]\n",
      "Epoch  14/30: 100%|██████████| 703/703 [00:03<00:00, 185.14batch/s, Loss = 0.6814, AUC = 91.94%]\n",
      "Epoch  15/30: 100%|██████████| 703/703 [00:03<00:00, 185.09batch/s, Loss = 0.7115, AUC = 93.08%]\n",
      "Epoch  16/30: 100%|██████████| 703/703 [00:03<00:00, 183.48batch/s, Loss = 0.6909, AUC = 92.97%]\n",
      "Epoch  17/30: 100%|██████████| 703/703 [00:03<00:00, 182.25batch/s, Loss = 0.7703, AUC = 92.46%]\n",
      "Epoch  18/30: 100%|██████████| 703/703 [00:03<00:00, 183.96batch/s, Loss = 0.8083, AUC = 88.23%]\n",
      "Epoch  19/30: 100%|██████████| 703/703 [00:03<00:00, 184.15batch/s, Loss = 0.8598, AUC = 90.53%]\n",
      "Epoch  20/30: 100%|██████████| 703/703 [00:04<00:00, 174.86batch/s, Loss = 0.8401, AUC = 84.30%]\n",
      "Epoch  21/30: 100%|██████████| 703/703 [00:03<00:00, 182.97batch/s, Loss = 0.9179, AUC = 87.45%]\n",
      "Epoch  22/30: 100%|██████████| 703/703 [00:03<00:00, 186.18batch/s, Loss = 0.9602, AUC = 86.37%]\n",
      "Epoch  23/30: 100%|██████████| 703/703 [00:03<00:00, 184.58batch/s, Loss = 1.0036, AUC = 85.21%]\n",
      "Epoch  24/30: 100%|██████████| 703/703 [00:03<00:00, 182.59batch/s, Loss = 1.0331, AUC = 88.56%]\n",
      "Epoch  25/30: 100%|██████████| 703/703 [00:03<00:00, 183.51batch/s, Loss = 1.1193, AUC = 89.42%]\n",
      "Epoch  26/30: 100%|██████████| 703/703 [00:03<00:00, 185.09batch/s, Loss = 1.2273, AUC = 89.12%]\n",
      "Epoch  27/30: 100%|██████████| 703/703 [00:03<00:00, 184.52batch/s, Loss = 1.1465, AUC = 90.36%]\n",
      "Epoch  28/30: 100%|██████████| 703/703 [00:03<00:00, 183.63batch/s, Loss = 1.2293, AUC = 89.22%]\n",
      "Epoch  29/30: 100%|██████████| 703/703 [00:03<00:00, 181.92batch/s, Loss = 1.2268, AUC = 89.79%]\n",
      "Epoch  30/30: 100%|██████████| 703/703 [00:03<00:00, 183.53batch/s, Loss = 1.3617, AUC = 87.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time = 119.52s, best validation AUC = 93.18% at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 781/781 [00:04<00:00, 169.00batch/s, Loss = 0.5277, AUC = 91.54%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array(0.5276519, dtype=float32),\n",
       " 91.53631861392364,\n",
       " array([0.        , 0.        , 0.        , ..., 0.99695951, 0.99711954,\n",
       "        1.        ]),\n",
       " array([0.00000000e+00, 8.00384184e-05, 3.28157516e-03, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(num_tokens=len(vocab), max_seq_len=512, num_classes=2, hidden_size=8, num_heads=2, num_transformer_blocks=4, mlp_hidden_size=4)\n",
    "train_and_evaluate(model, imdb_train_dataloader, imdb_val_dataloader, imdb_test_dataloader, num_classes=2, num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
